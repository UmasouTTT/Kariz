{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "from TCLIService.ttypes import TOperationState\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import ast\n",
    "import json\n",
    "import graph_tool.all as gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 query12\n",
      "\t 11 12\n",
      "1 query13\n",
      "\t 24 29\n",
      "2 query15\n",
      "\t 19 23\n",
      "3 query17\n",
      "\t 36 45\n",
      "4 query18\n",
      "\t 37 47\n",
      "5 query19\n",
      "\t 23 28\n",
      "6 query20\n",
      "\t 11 12\n",
      "7 query21\n",
      "\t 11 12\n",
      "8 query22\n",
      "\t 17 20\n",
      "9 query24\n",
      "\t 62 79\n",
      "10 query25\n",
      "\t 36 45\n",
      "11 query26\n",
      "\t 17 20\n",
      "12 query27\n",
      "\t 18 21\n",
      "13 query28\n",
      "\t 21 26\n",
      "14 query29\n",
      "\t 36 45\n",
      "15 query3\n",
      "\t 10 11\n",
      "16 query31\n",
      "\t 86 110\n",
      "17 query32\n",
      "\t 19 23\n",
      "18 query34\n",
      "\t 18 21\n",
      "19 query39\n",
      "\t 38 47\n",
      "20 query40\n",
      "\t 18 21\n",
      "21 query42\n",
      "\t 10 11\n",
      "22 query43\n",
      "\t 11 12\n",
      "23 query45\n",
      "\t 32 40\n",
      "24 query46\n",
      "\t 27 33\n",
      "25 query48\n",
      "\t 22 27\n",
      "26 query49\n",
      "\t 32 37\n",
      "27 query50\n",
      "\t 18 21\n",
      "28 query51\n",
      "\t 14 15\n",
      "29 query52\n",
      "\t 10 11\n",
      "30 query54\n",
      "\t 36 44\n",
      "31 query55\n",
      "\t 10 11\n",
      "32 query56\n",
      "\t 66 83\n",
      "33 query58\n",
      "\t 70 90\n",
      "34 query60\n",
      "\t 66 83\n",
      "35 query63\n",
      "\t 12 13\n",
      "36 query64\n",
      "\t 172 219\n",
      "37 query65\n",
      "\t 26 32\n",
      "38 query66\n",
      "\t 23 26\n",
      "39 query67\n",
      "\t 16 18\n",
      "40 query68\n",
      "\t 31 38\n",
      "41 query7\n",
      "\t 17 20\n",
      "42 query70\n",
      "\t 28 33\n",
      "43 query71\n",
      "\t 22 26\n",
      "44 query72\n",
      "\t 38 46\n",
      "45 query73\n",
      "\t 18 21\n",
      "47 query76\n",
      "\t 24 29\n",
      "48 query79\n",
      "\t 14 16\n",
      "49 query80\n",
      "\t 54 65\n",
      "50 query82\n",
      "\t 11 12\n",
      "51 query83\n",
      "\t 67 84\n",
      "52 query84\n",
      "\t 15 17\n",
      "53 query85\n",
      "\t 31 38\n",
      "54 query87\n",
      "\t 43 53\n",
      "55 query88\n",
      "\t 35 42\n",
      "56 query89\n",
      "\t 12 13\n",
      "57 query90\n",
      "\t 12 13\n",
      "58 query91\n",
      "\t 23 27\n",
      "59 query92\n",
      "\t 13 14\n",
      "60 query93\n",
      "\t 11 12\n",
      "61 query94\n",
      "\t 16 18\n",
      "62 query95\n",
      "\t 23 27\n",
      "63 query96\n",
      "\t 4 3\n",
      "64 query97\n",
      "\t 13 14\n",
      "65 query98\n",
      "\t 11 12\n"
     ]
    }
   ],
   "source": [
    "def is_float(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def parse_settings(setting):\n",
    "    prop, value = setting.split(' ')[1].split('=')\n",
    "    if value.isnumeric():\n",
    "        return prop, int(value)\n",
    "    elif is_float(value):\n",
    "        return prop, float(value)\n",
    "    return prop, value\n",
    "\n",
    "def apply(setting):\n",
    "    prop, value = setting\n",
    "    if isinstance(value, int):\n",
    "        query = 'set %s=%d'%(prop, value)\n",
    "    elif isinstance(value, float): \n",
    "        query = 'set %s=%f'%(prop, value)\n",
    "    else:\n",
    "        query = 'set %s=%s'%(prop, value)\n",
    "    cursor.execute(query, async=False)\n",
    " \n",
    "\n",
    "def process_tree(tree):\n",
    "    if not tree: return ''\n",
    "    for op in tree:\n",
    "        if 'children' not in tree[op]: \n",
    "            return op.replace(' Operator', '')\n",
    "        return op.replace(' Operator', '') + ' | ' + process_tree(tree[op]['children'])\n",
    "\n",
    "# Parse the insert query\n",
    "def parse_table_op_tree(table_tree, metadata):\n",
    "    for o in table_tree:\n",
    "        if o == 'TableScan':\n",
    "            if 'table:' in table_tree[o]:\n",
    "                metadata['in_table'] = table_tree[o]['table:']\n",
    "        metadata['ops'].append(o.replace(' Operator', ''))\n",
    "        if 'children' in table_tree[o]:\n",
    "            parse_table_op_tree(table_tree[o]['children'], metadata)\n",
    "\n",
    "\n",
    "def graph_annotation(annotations_tree):\n",
    "    exec_plan = annotations_tree\n",
    "   \n",
    "    metadata = {}\n",
    "    for stg_id in exec_plan:\n",
    "        stg_plan = exec_plan[stg_id]\n",
    "        for stg_op in stg_plan:\n",
    "            op_works = stg_plan[stg_op]\n",
    "\n",
    "            metadata[stg_id] = {'ops': []}\n",
    "            if stg_op == \"Stats-Aggr Operator\":\n",
    "                metadata[stg_id]['ops'].append(stg_op.replace(' Operator', ''))\n",
    "            elif stg_op == \"Move Operator\":\n",
    "                metadata[stg_id]['ops'].append(stg_op.replace(' Operator', ''))\n",
    "                metadata[stg_id]['out_table'] =  op_works['tables:']['table:'][\"name:\"]\n",
    "            elif stg_op in ['Map Reduce', 'Map Reduce Local Work']:        \n",
    "                for op in op_works:\n",
    "                    if op in [\"Alias -> Map Local Operator Tree:\", \"Alias -> Map Local Tables:\"]:\n",
    "                        op_tree = op_works[op]\n",
    "                        for table_name in op_tree:\n",
    "                            parse_table_op_tree(op_tree[table_name], metadata[stg_id])\n",
    "                    elif op == \"Map Operator Tree:\":\n",
    "                        op_tree = op_works[op][0]\n",
    "                        parse_table_op_tree(op_tree, metadata[stg_id])\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def build_graph_from_execution_plan(explain_cmd_json):\n",
    "    execution_plan = explain_cmd_json['STAGE DEPENDENCIES']\n",
    "    annotations_tree = graph_annotation(explain_cmd_json['STAGE PLANS'])\n",
    "\n",
    "    label_vid_map = {}\n",
    "    g = gt.Graph(directed=True)\n",
    "    g.vertex_properties['label'] = g.new_vertex_property(\"string\")\n",
    "    g.vertex_properties['ops'] = g.new_vertex_property(\"vector<string>\")\n",
    "    g.vertex_properties['ext_input'] = g.new_vertex_property(\"string\")\n",
    "    \n",
    "    for sid in execution_plan:\n",
    "        if sid not in label_vid_map:\n",
    "            vid = g.add_vertex()\n",
    "            label_vid_map[sid] = vid\n",
    "            g.vp.label[vid] = sid\n",
    "            g.vp.ops[vid] = annotations_tree[sid]['ops']\n",
    "            #print(annotations_tree[sid])\n",
    "        \n",
    "    for sid in execution_plan:\n",
    "        if 'DEPENDENT STAGES' in execution_plan[sid]:\n",
    "            dep_stages = [g.add_edge(label_vid_map[dsid], label_vid_map[sid]) for dsid in execution_plan[sid]['DEPENDENT STAGES'].replace(' ', '').split(',')]; dep_stages\n",
    "        if 'CONDITIONAL CHILD TASKS' in execution_plan[sid]:\n",
    "            dep_stages = [g.add_edge(label_vid_map[sid], label_vid_map[dsid]) for dsid in execution_plan[sid]['CONDITIONAL CHILD TASKS'].replace(' ', '').split(',')]; dep_stages\n",
    "    return g\n",
    "\n",
    "\n",
    "def main():\n",
    "    cursor = hive.connect('neu-3-1').cursor() \n",
    "\n",
    "    # hive configuration file\n",
    "    benchmark_path = '/local0/Kariz/expriments/benchmark/hive-testbench/sample-queries-tpcds'\n",
    "    queries = ['%s/%s'%(benchmark_path, f) for f in listdir(benchmark_path) if (f.endswith(\".sql\") and isfile(join(benchmark_path, f)))]\n",
    "    queries.sort()\n",
    "\n",
    "    with open('%s/testbench.settings'%(benchmark_path), 'r') as fd:\n",
    "        apply_settings = [apply(parse_settings(query)) for query in fd.read().split(';\\n')[:-1]]; apply_settings\n",
    "\n",
    "    query = 'use tpcds_bin_partitioned_textfile_10'\n",
    "    cursor.execute(query, async=False)\n",
    "\n",
    "    for index, query_file in enumerate(queries): \n",
    "        with open(query_file, 'r') as fd:\n",
    "            if index == 46:\n",
    "                continue;\n",
    "            print(index, query_file.split('/')[-1].split('.')[0])\n",
    "            query = 'EXPLAIN FORMATTED %s'%(fd.read().replace(';', ''))\n",
    "            cursor.execute(query, async=False)\n",
    "            res = cursor.fetchall()\n",
    "            explain_cmd_json = json.loads(res[0][0])\n",
    "            g = build_graph_from_execution_plan(explain_cmd_json)\n",
    "            print('\\t', g.num_vertices(), g.num_edges())\n",
    "            gt.graph_draw(g, vertex_text=g.vertex_index, \n",
    "                          output='%s.png'%(query_file.split('/')[-1].split('.')[0]),\n",
    "                          output_size=(1000, 1000))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
